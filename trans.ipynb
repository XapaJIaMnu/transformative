{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9\n",
      "Q torch.Size([10, 14, 512])\n",
      "Q before reshape torch.Size([10, 14, 512])\n",
      "Q after reshape torch.Size([10, 14, 64, 8])\n",
      "K permute torch.Size([10, 14, 8, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 8 at dimension 1, but got size 64 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cf6f1e825c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mfwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0mprintSent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTruth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cf6f1e825c72>\u001b[0m in \u001b[0;36mfwd\u001b[0;34m(self, oneHotTorch)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mtransformer_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_transformer_head_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIM_EMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mpostAttention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLinearProj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIM_EMB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cf6f1e825c72>\u001b[0m in \u001b[0;36madd_transformer_head_with_mask\u001b[0;34m(self, DIM_EMB, Batch, mask)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mK_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"K permute\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;31m#tmp1 = torch.t(K).matmul(Q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 8 at dimension 1, but got size 64 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import sin, cos\n",
    "from typing import List, Dict\n",
    "\n",
    "#FOR_GPU\n",
    "device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "RACHEL1 = \"<s> I hate cheese . </s>\"\n",
    "RACHEL2 = \"<s> Do you like cheese ? </s>\"\n",
    "RACHEL3 = \"<s> Where are the balloons ? </s>\"\n",
    "RACHEL4 = \"<s> A monkey ate my sandwich . </s>\"\n",
    "\n",
    "def getVocabDicts(sents: List[str]) -> (Dict[str, int], Dict[int, str]):\n",
    "    vocabDict = dict()\n",
    "    inv_vocab = dict()\n",
    "    i = 0\n",
    "    for sent in sents:\n",
    "        for token in sent.split():\n",
    "            if token not in vocabDict:\n",
    "                vocabDict[token] = i\n",
    "                i = i + 1\n",
    "    vocabDict[\"<unk>\"] = i\n",
    "    \n",
    "    for word in vocabDict:\n",
    "        inv_vocab[vocabDict[word]] = word\n",
    "    \n",
    "    return (vocabDict, inv_vocab)\n",
    "    \n",
    "\n",
    "SENTENCE = \"<s> I LIKE PIE VERY MUCH PIE . </s>\"\n",
    "SENTENCE2 = \"<s> WE ARE YOUNG . </s>\"\n",
    "SENTENCE3 = \"<s> LET US TEST HOW GOOD THIS IS . </s>\"\n",
    "SENTENCE4 = \"<s> TO SILENCE ALL THE HATERS OUT THERE . </s>\"\n",
    "SENTENCE5 = \"<s> AND FOR GLORY , FREEDOM , FREE PIE AND YOUNG LOVERS . </s>\"\n",
    "SENTENCE6 = \"<s> AND OF COURSE A HARD BOILED EGG IS ESSENTIAL . </s>\"\n",
    "DIM_EMB = 512\n",
    "torch.manual_seed(7)\n",
    "\n",
    "mysents = [SENTENCE, SENTENCE2, SENTENCE3, SENTENCE4, SENTENCE5, SENTENCE6]\n",
    "rachel_sents = [RACHEL1, RACHEL2, RACHEL3, RACHEL4]\n",
    "all_sents = rachel_sents + mysents\n",
    "vocab, inv_vocab = getVocabDicts(all_sents)\n",
    "\n",
    "DICT_SIZE = len(vocab)\n",
    "\n",
    "def getBatch(sents: List[str], vocab: Dict[str, int]) -> torch.Tensor:\n",
    "    #Get max sentence length\n",
    "    DICT_SIZE = len(vocab)\n",
    "    maxLen = 0\n",
    "    for sent in sents:\n",
    "        maxLen = max(maxLen, len(sent.split()))\n",
    "    oneHotBatch = np.zeros((len(sents), maxLen, DICT_SIZE), dtype=float)\n",
    "    for i in range(len(sents)):\n",
    "        curr_sent = sents[i].split()\n",
    "        for j in range(len(curr_sent)):\n",
    "            word = curr_sent[j]\n",
    "            oneHotBatch[i][j][vocab[word]] = 1\n",
    "    return torch.as_tensor(oneHotBatch, dtype=torch.float32, device=device)\n",
    "\n",
    "batch_tst = getBatch(all_sents, vocab)\n",
    "\n",
    "#oneHotSent = np.zeros((len(SENTENCE.split()), DICT_SIZE), dtype=float)\n",
    "\n",
    "#for i in range(len(SENTENCE.split())):\n",
    "#    word = SENTENCE.split()[i]\n",
    "#    oneHotSent[i][vocab[word]] = 1\n",
    "\n",
    "#oneHotTorch = torch.as_tensor(oneHotSent, dtype=torch.float32)\n",
    "    \n",
    "class FUCKYOU(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, max_sent_len, heads=8, head_dim=512):\n",
    "        super().__init__()\n",
    "        self.DICT_SIZE = vocab_size\n",
    "        self.batch_size = max_sent_len #@TODO CHANGE FIX THIS IS WRONG\n",
    "        self.transformerHeadConcat = 512 #@TODO CHANGE\n",
    "        self.heads = heads\n",
    "        self.HEAD_DIM = head_dim\n",
    "        \n",
    "        # In the transformer paper, the Wq, Wk and Wv are square 512x512\n",
    "        # and the number of heads is 512/64\n",
    "        \n",
    "        #TODO add biases because i don't have them\n",
    "        Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, self.HEAD_DIM), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wq = torch.nn.Parameter(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "        Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, self.HEAD_DIM), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wk = torch.nn.Parameter(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "        Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, self.HEAD_DIM), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wv = torch.nn.Parameter(Wv_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wo_skeleton = torch.tensor(np.zeros((self.transformerHeadConcat, DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wo = torch.nn.Parameter(Wo_skeleton, requires_grad=True)\n",
    "        \n",
    "        #Embeddings!\n",
    "        We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.We = torch.nn.Parameter(We_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wff_skeleton = torch.tensor(np.zeros((DIM_EMB, DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wff = torch.nn.Parameter(Wff_skeleton, requires_grad=True)\n",
    "        Bff_skeleton = torch.tensor(np.zeros((DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Bff = torch.nn.Parameter(Bff_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wproj_skeleton = torch.tensor(np.zeros((DIM_EMB, DICT_SIZE), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wproj = torch.nn.Parameter(Wproj_skeleton, requires_grad=True)\n",
    "        Bproj_skeleton = torch.tensor(np.zeros((DICT_SIZE), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Bproj = torch.nn.Parameter(Bproj_skeleton, requires_grad=True)\n",
    "        \n",
    "        Pos_emb = np.zeros((max_sent_len, DIM_EMB), dtype=float)\n",
    "        for p in range(4): #should max_sent_len but 9 works better. Go figure\n",
    "            for i in range(DIM_EMB//2):\n",
    "                val = p/(10000**(2*i/DIM_EMB))\n",
    "                Pos_emb[p][2*i] = sin(val)\n",
    "                Pos_emb[p][2*i + 1] = cos(val)\n",
    "\n",
    "        self.Pos_emb_var = torch.nn.Parameter(torch.tensor(Pos_emb, dtype=torch.float32, device=device), requires_grad=True)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.0001)\n",
    "\n",
    "        \n",
    "    \n",
    "    #DATA PREPARATION DONE\n",
    "\n",
    "    # Create self attention\n",
    "    # also need to add a mask here\n",
    "    # @TODO Add a Bias to all Wsz\n",
    "    def add_transformer_head_with_mask(self, DIM_EMB, Batch, mask):\n",
    "        Q = Batch.matmul(self.Wq)\n",
    "        V = Batch.matmul(self.Wv)\n",
    "        K = Batch.matmul(self.Wk)\n",
    "        print(\"Q\", Q.shape)\n",
    "        num_sents = Q.shape[0] #batch_size 10\n",
    "        sent_len = Q.shape[1] # 14\n",
    "        dim_q = Q.shape[2] # 512\n",
    "        \n",
    "        print(\"Q before reshape\", Q.shape)\n",
    "        Q = Q.permute(0, 2, 1)\n",
    "        V = V.permute(0, 2, 1)\n",
    "        K = K.permute(0, 2, 1)\n",
    "        Q = Q.reshape(num_sents, sent_len, dim_q//self.heads, -1)\n",
    "        V = V.reshape(num_sents, sent_len, dim_q//self.heads, -1)\n",
    "        K = K.reshape(num_sents, sent_len, dim_q//self.heads, -1)\n",
    "        print(\"Q after reshape\", Q.shape)\n",
    "\n",
    "        tmp1 = Q.matmul(K.permute(0, 1, 3, 2))\n",
    "        #tmp1 = torch.t(K).matmul(Q)\n",
    "\n",
    "        tmp = tmp1/(64**(1/2))\n",
    "        #@TODO this might be wrong for dim=-1\n",
    "        softM = torch.nn.Softmax(dim=-1)\n",
    "        tmp = tmp.masked_fill(mask == 0, -1e9)\n",
    "        #print(\"AFTER MASK\", tmp)\n",
    "        a = softM(tmp)\n",
    "        #print(\"AFTER SOFTMAX\", a)\n",
    "        z = a.matmul(V)\n",
    "        #print(\"After Z*V\", z)\n",
    "        return z\n",
    "\n",
    "    # @TODO add a bias to Wo\n",
    "    def addLinearProj(self, concatTransHead, DIM_EMB):\n",
    "        '''Already concatenataed all heads'''\n",
    "\n",
    "        return concatTransHead.matmul(self.Wo)\n",
    "\n",
    "    #Normalize\n",
    "    def layerNorm(self, mat):\n",
    "        #@TODO there should be a square root here somewhere\n",
    "        return (mat - mat.mean(dim=1, keepdim=True)) / (mat.std(dim=1, keepdim=True))\n",
    "\n",
    "    EPOCHS=10\n",
    "    def fwd(self, oneHotTorch: torch.Tensor):\n",
    "        \n",
    "        Batch = oneHotTorch.matmul(self.We)\n",
    "        Batch = Batch*(DIM_EMB**(1/2))\n",
    "        \n",
    "        #Positional embeddings here\n",
    "        Batch = Batch + self.Pos_emb_var\n",
    "        #Mask the batch post pos_emb\n",
    "        mask1 = torch.sum(oneHotTorch, dim=2) > 0\n",
    "        mask1 = mask1.reshape(Batch.shape[0], Batch.shape[1], 1).float()\n",
    "        \n",
    "        Batch = Batch*mask1\n",
    "\n",
    "        #Create MASK HERE\n",
    "        #\n",
    "        #print(Batch.shape)\n",
    "        mask = np.triu(np.ones((Batch.shape[1], Batch.shape[1])), k=1)\n",
    "        mask = torch.tensor(mask, device=device) == 0\n",
    "\n",
    "        transformer_head = self.add_transformer_head_with_mask(DIM_EMB, Batch, mask)\n",
    "        postAttention = self.addLinearProj(transformer_head, DIM_EMB)\n",
    "\n",
    "        # Add residual connection to the embeddings\n",
    "        postAttention = postAttention + Batch\n",
    "        #print(postAttention)\n",
    "\n",
    "        normalizedPostAttention = self.layerNorm(postAttention)\n",
    "        #print(normalizedPostAttention)\n",
    "        # @TODO no dropout\n",
    "        # Take the output of the attention and do a one layer FF that outputs size of\n",
    "        activation = torch.nn.ReLU()\n",
    "        postFF = activation(normalizedPostAttention.matmul(self.Wff) + self.Bff)\n",
    "\n",
    "        #Residual\n",
    "        preSoftmaxProj = self.layerNorm(postFF + normalizedPostAttention)\n",
    "\n",
    "        linearLay = preSoftmaxProj.matmul(self.Wproj) + self.Bproj\n",
    "        return linearLay\n",
    "\n",
    "    #@TODO\n",
    "    def getOneHot(self, truth):\n",
    "        ret = torch.Tensor(self.batch_size).long() \n",
    "\n",
    "model = FUCKYOU(DICT_SIZE, max([len(x.split()) for x in all_sents]))\n",
    "print(np.mean([len(x.split()) for x in all_sents]))\n",
    "EPOCHS=1\n",
    "\n",
    "def printSent(softmaxd, vocab, batch_onehot):\n",
    "    words = [[]]\n",
    "    for j in range(len(softmaxd)):\n",
    "        words.append([])\n",
    "        for i in range(len(softmaxd[j])):\n",
    "            _, idx = torch.max(softmaxd[j][i], 0)\n",
    "            hasWord, _ = torch.max(batch_onehot[j][i], 0)\n",
    "            if hasWord == 1:\n",
    "                words[j].append(vocab[int(idx)])\n",
    "    for arr in words:\n",
    "        print(\" \".join(arr))\n",
    "    \n",
    "def getTruth(oneHotSent):\n",
    "    truth = []\n",
    "    for j in range(len(oneHotSent)):\n",
    "        truth.append([])\n",
    "        for i in range(len(oneHotSent[j])):\n",
    "            _, idx = torch.max(oneHotSent[j][i], 0)\n",
    "            truth[j].append(idx)\n",
    "    return torch.tensor(truth, device=device).long()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.optimizer.zero_grad()\n",
    "    fwd = model.fwd(batch_tst)\n",
    "    printSent(fwd, inv_vocab, batch_tst)\n",
    "    truth = getTruth(batch_tst)\n",
    "    loss = torch.nn.functional.cross_entropy(fwd.permute(0, 2, 1), truth, size_average = True, ignore_index=11)\n",
    "    print(\"Loss: \", loss.item())\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
