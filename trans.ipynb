{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, 'I': 1, 'LIKE': 2, 'PIE': 3, 'VERY': 4, 'MUCH': 5, '.': 6, '<unk>': 7, 'not_a_word': 8}\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n",
      "torch.Size([8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import sin, cos\n",
    "\n",
    "SENTENCE = \"<s> I LIKE PIE VERY MUCH PIE .\"\n",
    "DIM_EMB = 512\n",
    "torch.manual_seed(7)\n",
    "\n",
    "vocab = dict()\n",
    "i = 0\n",
    "for token in SENTENCE.split():\n",
    "    if token not in vocab:\n",
    "        vocab[token] = i\n",
    "        i += 1\n",
    "vocab[\"<unk>\"] = i\n",
    "vocab[\"not_a_word\"] = i + 1\n",
    "print(vocab)\n",
    "\n",
    "DICT_SIZE = len(vocab)\n",
    "\n",
    "oneHotSent = np.zeros((len(SENTENCE.split()), DICT_SIZE), dtype=float)\n",
    "\n",
    "for i in range(len(SENTENCE.split())):\n",
    "    word = SENTENCE.split()[i]\n",
    "    oneHotSent[i][vocab[word]] = 1\n",
    "\n",
    "oneHotTorch = torch.as_tensor(oneHotSent, dtype=torch.float32)\n",
    "    \n",
    "class FUCKYOU(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.DICT_SIZE = vocab_size\n",
    "        \n",
    "        Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        Wq = torch.nn.Parameter(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "        Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        Wk = torch.nn.Parameter(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "        Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        Wv = torch.nn.Parameter(Wv_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wo_skeleton = torch.tensor(np.zeros((concatTransHead.shape[1], DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        Wo = torch.nn.Parameter(Wo_skeleton, requires_grad=True)\n",
    "        \n",
    "        We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        We = torch.nn.Parameter(We_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wff_skeleton = torch.tensor(np.zeros((DIM_EMB, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        Wff = torch.nn.Parameter(Wff_skeleton, requires_grad=True)\n",
    "        Bff_skeleton = torch.tensor(np.zeros((DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        Bff = torch.nn.Parameter(Bff_skeleton, requires_grad=True)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "#DATA PREPARATION DONE\n",
    "\n",
    "# Create self attention\n",
    "# also need to add a mask here\n",
    "# @TODO Add a Bias to all Wsz\n",
    "def add_transformer_head_with_mask(DIM_EMB, Batch, mask):\n",
    "    Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wq = torch.nn.Parameter(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "    Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wk = torch.nn.Parameter(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "    Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wv = torch.nn.Parameter(Wv_skeleton, requires_grad=True)\n",
    "\n",
    "    Q = Batch.mm(Wq)\n",
    "    V = Batch.mm(Wv)\n",
    "    K = Batch.mm(Wk)\n",
    "\n",
    "    tmp1 = Q.mm(torch.t(K))\n",
    "\n",
    "    tmp = tmp1/(64**(1/2))\n",
    "    #print(tmp)\n",
    "\n",
    "    softM = torch.nn.Softmax(dim=-1)\n",
    "    #apply maks here\n",
    "    tmp = tmp.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    a = softM(tmp)\n",
    "\n",
    "    z = a.mm(V)\n",
    "    return z\n",
    "\n",
    "# @TODO add a bias to Wo\n",
    "def addLinearProj(concatTransHead, DIM_EMB):\n",
    "    '''Already concatenataed all heads'''\n",
    "    Wo_skeleton = torch.tensor(np.zeros((concatTransHead.shape[1], DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wo = torch.nn.Parameter(Wo_skeleton, requires_grad=True)\n",
    "\n",
    "    return concatTransHead.mm(Wo)\n",
    "\n",
    "#Normalize\n",
    "def layerNorm(mat):\n",
    "    return (mat - mat.mean(dim=0, keepdim=True)) / (mat.std(dim=0, keepdim=True))\n",
    "\n",
    "EPOCHS=10\n",
    "def fwd():\n",
    "    We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    We = torch.nn.Parameter(We_skeleton, requires_grad=True)\n",
    "\n",
    "    Batch = oneHotTorch.mm(We)\n",
    "    Pos_emb = np.zeros(Batch.shape, dtype=float)\n",
    "    for p in range(len(SENTENCE.split())):\n",
    "        for i in range(DIM_EMB//2):\n",
    "            val = p/(10000**(2*i/DIM_EMB))\n",
    "            Pos_emb[p][2*i] = sin(val)\n",
    "            Pos_emb[p][2*i + 1] = cos(val)\n",
    "\n",
    "    Pos_emb_var = torch.nn.Parameter(torch.tensor(Pos_emb, dtype=torch.float32), requires_grad=False)\n",
    "\n",
    "    Batch = Batch*(DIM_EMB**(1/2))\n",
    "\n",
    "    Batch = Batch + Pos_emb_var\n",
    "\n",
    "    #Batch (in this case a one sentence batch) is prepared. Maxi calls this embedded sentence\n",
    "\n",
    "    #Create MASK HERE\n",
    "    #\n",
    "    mask = np.triu(np.ones((len(SENTENCE.split()), len(SENTENCE.split()))), k=1)\n",
    "    mask = torch.from_numpy(mask) == 0\n",
    "\n",
    "    transformer_head = add_transformer_head_with_mask(DIM_EMB, Batch, mask)\n",
    "    postAttention = addLinearProj(transformer_head, DIM_EMB)\n",
    "\n",
    "    # Add residual connection to the embeddings\n",
    "    postAttention = postAttention + Batch\n",
    "\n",
    "    normalizedPostAttention = layerNorm(postAttention)\n",
    "\n",
    "    # @TODO no dropout\n",
    "    # Take the output of the attention and do a one layer FF that outputs size of\n",
    "    Wff_skeleton = torch.tensor(np.zeros((DIM_EMB, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wff = torch.nn.Parameter(Wff_skeleton, requires_grad=True)\n",
    "    Bff_skeleton = torch.tensor(np.zeros((DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Bff = torch.nn.Parameter(Bff_skeleton, requires_grad=True)\n",
    "\n",
    "    activation = torch.nn.ReLU()\n",
    "    postFF = activation(normalizedPostAttention.mm(Wff) + Bff)\n",
    "\n",
    "    #Residual\n",
    "    preSoftmaxProj = layerNorm(postFF + normalizedPostAttention)\n",
    "\n",
    "    #Projexct\n",
    "    Wproj_skeleton = torch.tensor(np.zeros((DIM_EMB, DICT_SIZE), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wproj = torch.nn.Parameter(Wproj_skeleton, requires_grad=True)\n",
    "    Bproj_skeleton = torch.tensor(np.zeros((DICT_SIZE), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Bproj = torch.nn.Parameter(Bproj_skeleton, requires_grad=True)\n",
    "\n",
    "    linearLay = activation(preSoftmaxProj.mm(Wproj) + Bproj)\n",
    "\n",
    "    #Softmax\n",
    "    softmax = torch.nn.LogSoftmax(dim=1)\n",
    "    output = softmax(linearLay)\n",
    "    return output\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    truth = oneHotTorch #SAKA [:, 1:].contiguous().view(-1)\n",
    "    print(truth.shape)\n",
    "    #print(truth)\n",
    "    print(fwd().shape)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
