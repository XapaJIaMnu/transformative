{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'LIKE': 1, 'PIE': 2, 'VERY': 3, 'MUCH': 4, '.': 5, '<unk>': 6, '<s>': 7}\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\n",
      "EWADSDASDASDASDASDASDASDASDAS\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 1.3456e+03, -6.2462e+02,  2.6924e+02,  3.2292e+02,  5.9020e+02,\n",
      "          2.8053e+02, -5.8125e+02, -7.0534e+02,  1.7737e+02, -3.4275e+02,\n",
      "          2.5205e+02,  4.5682e+02, -2.4706e+02,  2.5111e+02, -1.0888e+03,\n",
      "          5.9780e+02, -3.1436e+02,  4.6598e+02, -6.4631e+02,  6.1107e+01,\n",
      "          2.8097e+02,  6.1181e+02, -5.4264e+02, -3.3123e+02,  3.5296e+02,\n",
      "         -7.4572e+02, -6.8368e+02, -4.7904e+02,  7.7429e+02,  2.0400e+02,\n",
      "          1.3555e+03,  3.0731e+01, -1.1012e+02, -1.4150e+02,  1.5600e+02,\n",
      "         -2.2361e+02, -3.9965e+02,  4.9752e+02, -4.1237e+02,  4.2118e+02,\n",
      "         -7.2110e+02, -7.5743e+01, -3.6375e+01, -8.8165e+02,  4.1749e+02,\n",
      "         -4.7011e+02,  7.0018e-01, -1.6454e+02, -6.5178e+02,  6.0629e+02,\n",
      "         -8.3159e+02,  4.6517e+02, -3.2038e+02, -7.7703e+02, -1.3042e+01,\n",
      "         -2.8963e+02, -4.2643e+02, -9.6636e+01, -2.1374e+02, -8.3569e+01,\n",
      "          5.1524e+01,  6.9562e+02,  4.8997e+02,  5.0321e+02],\n",
      "        [ 1.3456e+03, -6.2462e+02,  2.6924e+02,  3.2292e+02,  5.9020e+02,\n",
      "          2.8053e+02, -5.8125e+02, -7.0534e+02,  1.7737e+02, -3.4275e+02,\n",
      "          2.5205e+02,  4.5682e+02, -2.4706e+02,  2.5111e+02, -1.0888e+03,\n",
      "          5.9780e+02, -3.1436e+02,  4.6598e+02, -6.4631e+02,  6.1107e+01,\n",
      "          2.8097e+02,  6.1181e+02, -5.4264e+02, -3.3123e+02,  3.5296e+02,\n",
      "         -7.4572e+02, -6.8368e+02, -4.7904e+02,  7.7429e+02,  2.0400e+02,\n",
      "          1.3555e+03,  3.0731e+01, -1.1012e+02, -1.4150e+02,  1.5600e+02,\n",
      "         -2.2361e+02, -3.9965e+02,  4.9752e+02, -4.1237e+02,  4.2118e+02,\n",
      "         -7.2110e+02, -7.5743e+01, -3.6375e+01, -8.8165e+02,  4.1749e+02,\n",
      "         -4.7011e+02,  7.0018e-01, -1.6454e+02, -6.5178e+02,  6.0629e+02,\n",
      "         -8.3159e+02,  4.6517e+02, -3.2038e+02, -7.7703e+02, -1.3042e+01,\n",
      "         -2.8963e+02, -4.2643e+02, -9.6636e+01, -2.1374e+02, -8.3569e+01,\n",
      "          5.1524e+01,  6.9562e+02,  4.8997e+02,  5.0321e+02],\n",
      "        [-7.8739e+01, -3.9120e+02, -6.2677e+02,  3.1113e+01,  4.7647e+02,\n",
      "          9.6394e+01, -4.7709e+02,  4.9356e+02, -8.0682e+02, -3.8662e+02,\n",
      "          1.7899e+02,  1.2395e+03,  8.4895e+01, -8.1158e+02, -9.1213e+02,\n",
      "          8.3332e+02, -9.8097e+02,  6.0106e+02,  4.3770e+02,  7.8958e+01,\n",
      "          4.8874e+01, -4.2257e+02,  7.2855e+02, -1.6075e+02,  5.7691e+01,\n",
      "          5.7029e+02, -2.7573e+02, -6.3799e+02, -3.0578e+02, -1.8151e+02,\n",
      "         -7.8619e+02,  6.7934e+02,  2.7137e+02,  3.1675e+02, -6.0324e+02,\n",
      "         -1.3046e+03, -5.3971e+02, -5.2339e+02,  2.1821e+02, -5.2381e+02,\n",
      "          2.6967e+02, -4.9645e+02, -3.2293e+02, -2.2443e+02, -5.0226e+02,\n",
      "          8.7184e+02, -5.2321e+02,  2.2598e+02, -9.2337e+02, -6.6869e+02,\n",
      "          3.1334e+02, -2.2480e+02,  9.9084e+01, -6.0033e+02, -3.8108e+02,\n",
      "          3.7637e+02, -3.3797e+02, -2.4888e+02,  1.4901e+02,  1.9830e+02,\n",
      "          7.2417e+01,  7.8056e+02,  1.1321e+03, -5.9450e+02],\n",
      "        [-7.8739e+01, -3.9120e+02, -6.2677e+02,  3.1113e+01,  4.7647e+02,\n",
      "          9.6394e+01, -4.7709e+02,  4.9356e+02, -8.0682e+02, -3.8662e+02,\n",
      "          1.7899e+02,  1.2395e+03,  8.4895e+01, -8.1158e+02, -9.1213e+02,\n",
      "          8.3332e+02, -9.8097e+02,  6.0106e+02,  4.3770e+02,  7.8958e+01,\n",
      "          4.8874e+01, -4.2257e+02,  7.2855e+02, -1.6075e+02,  5.7691e+01,\n",
      "          5.7029e+02, -2.7573e+02, -6.3799e+02, -3.0578e+02, -1.8151e+02,\n",
      "         -7.8619e+02,  6.7934e+02,  2.7137e+02,  3.1675e+02, -6.0324e+02,\n",
      "         -1.3046e+03, -5.3971e+02, -5.2339e+02,  2.1821e+02, -5.2381e+02,\n",
      "          2.6967e+02, -4.9645e+02, -3.2293e+02, -2.2443e+02, -5.0226e+02,\n",
      "          8.7184e+02, -5.2321e+02,  2.2598e+02, -9.2337e+02, -6.6869e+02,\n",
      "          3.1334e+02, -2.2480e+02,  9.9084e+01, -6.0033e+02, -3.8108e+02,\n",
      "          3.7637e+02, -3.3797e+02, -2.4888e+02,  1.4901e+02,  1.9830e+02,\n",
      "          7.2417e+01,  7.8056e+02,  1.1321e+03, -5.9450e+02],\n",
      "        [-3.2127e+02, -8.3065e+02,  3.9494e+02, -4.1433e+02, -2.5097e+02,\n",
      "          2.3831e+02,  1.2867e+02,  2.0979e+02, -6.1661e+02,  1.0382e+03,\n",
      "         -2.4501e+02, -2.3227e+02, -6.6046e+02, -5.9702e+02, -6.2176e+02,\n",
      "          3.5675e+02,  2.0452e+02, -4.1776e+02, -4.2995e+00, -9.7362e+02,\n",
      "         -2.8989e+02,  8.0708e+02, -6.4379e+01,  3.1150e+01, -3.7955e+02,\n",
      "         -3.2251e+02,  5.4285e+02, -4.3814e+02,  1.8868e+02, -1.3142e+03,\n",
      "         -3.4616e+02,  2.5439e+01, -2.4551e+02,  5.4438e+02,  3.4280e+01,\n",
      "         -3.0896e+02, -4.0015e+02,  3.3142e+02, -4.2332e+02, -3.9891e+02,\n",
      "         -7.1853e+02, -1.0627e+02, -3.9437e+01,  2.2712e+02, -5.3191e+01,\n",
      "         -2.0282e+02, -3.5842e+01, -1.0233e+03,  9.0576e+02, -5.6012e+02,\n",
      "         -1.7274e+02,  4.0907e+02,  9.6723e+02,  4.0127e+02, -1.1586e+03,\n",
      "          2.4358e+02, -4.9592e+02,  7.0374e+01,  3.4318e+02, -1.9264e+01,\n",
      "          8.7636e+02,  5.1099e+02,  4.3178e+02,  1.5761e+02],\n",
      "        [-7.0828e+01, -3.8878e+02, -6.3517e+02,  3.2847e+01,  4.6965e+02,\n",
      "          1.0898e+02, -4.7845e+02,  5.0161e+02, -7.7387e+02, -4.1411e+02,\n",
      "          1.7609e+02,  1.2299e+03,  8.3624e+01, -8.0538e+02, -8.9947e+02,\n",
      "          8.1877e+02, -9.7115e+02,  6.0953e+02,  4.2904e+02,  6.5044e+01,\n",
      "          2.8764e+01, -4.2273e+02,  7.3934e+02, -1.5419e+02,  7.0163e+01,\n",
      "          5.6216e+02, -2.9384e+02, -6.2792e+02, -3.0770e+02, -1.7089e+02,\n",
      "         -7.7381e+02,  6.6911e+02,  2.8478e+02,  2.9385e+02, -6.0467e+02,\n",
      "         -1.2883e+03, -5.4349e+02, -5.3294e+02,  2.1735e+02, -5.2008e+02,\n",
      "          2.5419e+02, -4.8872e+02, -3.2716e+02, -2.0935e+02, -5.0513e+02,\n",
      "          8.6204e+02, -5.2571e+02,  2.0310e+02, -9.0279e+02, -6.8302e+02,\n",
      "          3.0411e+02, -2.2445e+02,  1.0651e+02, -6.1021e+02, -3.9478e+02,\n",
      "          3.8134e+02, -3.2297e+02, -2.5743e+02,  1.4076e+02,  1.9286e+02,\n",
      "          8.0802e+01,  7.7299e+02,  1.1297e+03, -5.9337e+02],\n",
      "        [ 1.3456e+03, -6.2462e+02,  2.6924e+02,  3.2292e+02,  5.9020e+02,\n",
      "          2.8053e+02, -5.8125e+02, -7.0534e+02,  1.7737e+02, -3.4275e+02,\n",
      "          2.5205e+02,  4.5682e+02, -2.4706e+02,  2.5111e+02, -1.0888e+03,\n",
      "          5.9780e+02, -3.1436e+02,  4.6598e+02, -6.4631e+02,  6.1107e+01,\n",
      "          2.8097e+02,  6.1181e+02, -5.4264e+02, -3.3123e+02,  3.5296e+02,\n",
      "         -7.4572e+02, -6.8368e+02, -4.7904e+02,  7.7429e+02,  2.0400e+02,\n",
      "          1.3555e+03,  3.0731e+01, -1.1012e+02, -1.4150e+02,  1.5600e+02,\n",
      "         -2.2361e+02, -3.9965e+02,  4.9752e+02, -4.1237e+02,  4.2118e+02,\n",
      "         -7.2110e+02, -7.5743e+01, -3.6375e+01, -8.8165e+02,  4.1749e+02,\n",
      "         -4.7011e+02,  7.0018e-01, -1.6454e+02, -6.5178e+02,  6.0629e+02,\n",
      "         -8.3159e+02,  4.6517e+02, -3.2038e+02, -7.7703e+02, -1.3042e+01,\n",
      "         -2.8963e+02, -4.2643e+02, -9.6636e+01, -2.1374e+02, -8.3569e+01,\n",
      "          5.1524e+01,  6.9562e+02,  4.8997e+02,  5.0321e+02]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import sin, cos\n",
    "\n",
    "SENTENCE = \"I LIKE PIE VERY MUCH PIE .\"\n",
    "DIM_EMB = 512\n",
    "torch.manual_seed(7)\n",
    "\n",
    "vocab = dict()\n",
    "i = 0\n",
    "for token in SENTENCE.split():\n",
    "    if token not in vocab:\n",
    "        vocab[token] = i\n",
    "        i += 1\n",
    "vocab[\"<unk>\"] = i\n",
    "vocab[\"<s>\"] = i + 1\n",
    "print(vocab)\n",
    "\n",
    "DICT_SIZE = len(vocab)\n",
    "\n",
    "oneHotSent = np.zeros((len(SENTENCE.split()), DICT_SIZE), dtype=float)\n",
    "\n",
    "for i in range(len(SENTENCE.split())):\n",
    "    word = SENTENCE.split()[i]\n",
    "    oneHotSent[i][vocab[word]] = 1\n",
    "\n",
    "oneHotTorch = torch.as_tensor(oneHotSent, dtype=torch.float32)\n",
    "    \n",
    "#print(oneHotTorch)\n",
    "#print(oneHotTorch.shape)\n",
    "#print(oneHotTorch.dtype)\n",
    "\n",
    "#DATA PREPARATION DONE\n",
    "\n",
    "We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "We = torch.autograd.Variable(We_skeleton, requires_grad=True)\n",
    "#print(We)\n",
    "#print(We.shape)\n",
    "#print(We.dtype)\n",
    "\n",
    "\n",
    "Batch = oneHotTorch.mm(We)\n",
    "Pos_emb = np.zeros(Batch.shape, dtype=float)\n",
    "for p in range(len(SENTENCE.split())):\n",
    "    for i in range(DIM_EMB//2):\n",
    "        val = p/(10000**(2*i/DIM_EMB))\n",
    "        Pos_emb[p][2*i] = sin(val)\n",
    "        Pos_emb[p][2*i + 1] = cos(val)\n",
    "\n",
    "Pos_emb_var = torch.autograd.Variable(torch.tensor(Pos_emb, dtype=torch.float32), requires_grad=False)\n",
    "#print(Pos_emb_var)\n",
    "\n",
    "#print(Batch)\n",
    "#Scale batch up\n",
    "Batch = Batch*(DIM_EMB**(1/2))\n",
    "#print(Batch)\n",
    "Batch = Batch + Pos_emb_var\n",
    "#print(Batch)\n",
    "\n",
    "#print(Batch.shape)\n",
    "#print(Batch.dtype)\n",
    "\n",
    "#Batch (in this case a one sentence batch) is prepared. Maxi calls this embedded sentence\n",
    "\n",
    "#Create MASK HERE\n",
    "#\n",
    "mask = np.triu(np.ones((len(SENTENCE.split()), len(SENTENCE.split()))), k=1)\n",
    "mask = torch.from_numpy(mask) == 0\n",
    "print(mask)\n",
    "\n",
    "# Create self attention\n",
    "# also need to add a mask here\n",
    "def add_transformer_head_with_mask(DIM_EMB, Batch, mask):\n",
    "    Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wq = torch.autograd.Variable(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "    Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wk = torch.autograd.Variable(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "    Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wv = torch.autograd.Variable(Wv_skeleton, requires_grad=True)\n",
    "\n",
    "    Q = Batch.mm(Wq)\n",
    "    V = Batch.mm(Wv)\n",
    "    K = Batch.mm(Wk)\n",
    "    \n",
    "    tmp1 = Q.mm(torch.t(K))\n",
    "\n",
    "    tmp = tmp1/(64**(1/2))\n",
    "    #print(tmp)\n",
    "    \n",
    "    softM = torch.nn.Softmax(dim=-1)\n",
    "    #apply maks here\n",
    "    tmp = tmp.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    #print(tmp)\n",
    "\n",
    "    \n",
    "    a = softM(tmp)\n",
    "    print(a)\n",
    "\n",
    "    z = a.mm(V)\n",
    "    return z\n",
    "\n",
    "\n",
    "print(\"EWADSDASDASDASDASDASDASDASDAS\")\n",
    "print(add_transformer_head_with_mask(DIM_EMB, Batch, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
