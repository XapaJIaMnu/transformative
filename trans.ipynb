{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, 'I': 1, 'LIKE': 2, 'PIE': 3, 'VERY': 4, 'MUCH': 5, '.': 6, '<unk>': 7, 'Peter': 8}\n",
      "<s> VERY LIKE VERY PIE <s> VERY <s>\n",
      "Loss:  17.468599319458008\n",
      "<s> I LIKE I PIE <s> VERY <s>\n",
      "Loss:  9.194619178771973\n",
      ". PIE LIKE PIE <s> . VERY .\n",
      "Loss:  5.541458606719971\n",
      ". PIE LIKE PIE <s> . PIE .\n",
      "Loss:  3.3299055099487305\n",
      ". I LIKE I Peter . PIE .\n",
      "Loss:  2.780008316040039\n",
      "VERY I LIKE I Peter VERY PIE VERY\n",
      "Loss:  2.9167559146881104\n",
      "<s> PIE LIKE PIE Peter <s> PIE <s>\n",
      "Loss:  2.917109966278076\n",
      "<s> I LIKE PIE Peter <s> PIE <s>\n",
      "Loss:  2.602118968963623\n",
      "<s> I LIKE I Peter <s> PIE <s>\n",
      "Loss:  2.1098036766052246\n",
      ". I LIKE I Peter . PIE .\n",
      "Loss:  2.078972816467285\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import sin, cos\n",
    "\n",
    "SENTENCE = \"<s> I LIKE PIE VERY MUCH PIE .\"\n",
    "DIM_EMB = 512\n",
    "torch.manual_seed(7)\n",
    "\n",
    "vocab = dict()\n",
    "i = 0\n",
    "for token in SENTENCE.split():\n",
    "    if token not in vocab:\n",
    "        vocab[token] = i\n",
    "        i += 1\n",
    "vocab[\"<unk>\"] = i\n",
    "vocab[\"Peter\"] = i + 1\n",
    "print(vocab)\n",
    "\n",
    "inv_vocab = dict()\n",
    "for word in vocab:\n",
    "    inv_vocab[vocab[word]] = word\n",
    "\n",
    "DICT_SIZE = len(vocab)\n",
    "\n",
    "oneHotSent = np.zeros((len(SENTENCE.split()), DICT_SIZE), dtype=float)\n",
    "\n",
    "for i in range(len(SENTENCE.split())):\n",
    "    word = SENTENCE.split()[i]\n",
    "    oneHotSent[i][vocab[word]] = 1\n",
    "\n",
    "oneHotTorch = torch.as_tensor(oneHotSent, dtype=torch.float32)\n",
    "    \n",
    "class FUCKYOU(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.DICT_SIZE = vocab_size\n",
    "        self.batch_size = batch_size #@TODO CHANGE\n",
    "        self.transformerHeadConcat = 64 #@TODO CHANGE\n",
    "        \n",
    "        Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Wq = torch.nn.Parameter(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "        Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Wk = torch.nn.Parameter(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "        Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Wv = torch.nn.Parameter(Wv_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wo_skeleton = torch.tensor(np.zeros((self.transformerHeadConcat, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Wo = torch.nn.Parameter(Wo_skeleton, requires_grad=True)\n",
    "        \n",
    "        We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.We = torch.nn.Parameter(We_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wff_skeleton = torch.tensor(np.zeros((DIM_EMB, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Wff = torch.nn.Parameter(Wff_skeleton, requires_grad=True)\n",
    "        Bff_skeleton = torch.tensor(np.zeros((DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Bff = torch.nn.Parameter(Bff_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wproj_skeleton = torch.tensor(np.zeros((DIM_EMB, DICT_SIZE), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Wproj = torch.nn.Parameter(Wproj_skeleton, requires_grad=True)\n",
    "        Bproj_skeleton = torch.tensor(np.zeros((DICT_SIZE), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "        self.Bproj = torch.nn.Parameter(Bproj_skeleton, requires_grad=True)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        \n",
    "    \n",
    "    #DATA PREPARATION DONE\n",
    "\n",
    "    # Create self attention\n",
    "    # also need to add a mask here\n",
    "    # @TODO Add a Bias to all Wsz\n",
    "    def add_transformer_head_with_mask(self, DIM_EMB, Batch, mask):\n",
    "        Q = Batch.mm(self.Wq)\n",
    "        V = Batch.mm(self.Wv)\n",
    "        K = Batch.mm(self.Wk)\n",
    "\n",
    "        tmp1 = Q.mm(torch.t(K))\n",
    "\n",
    "        tmp = tmp1/(64**(1/2))\n",
    "        #print(tmp)\n",
    "\n",
    "        softM = torch.nn.Softmax(dim=-1)\n",
    "        #apply maks here\n",
    "        tmp = tmp.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        a = softM(tmp)\n",
    "\n",
    "        z = a.mm(V)\n",
    "        return z\n",
    "\n",
    "    # @TODO add a bias to Wo\n",
    "    def addLinearProj(self, concatTransHead, DIM_EMB):\n",
    "        '''Already concatenataed all heads'''\n",
    "\n",
    "        return concatTransHead.mm(self.Wo)\n",
    "\n",
    "    #Normalize\n",
    "    def layerNorm(self, mat):\n",
    "        return (mat - mat.mean(dim=0, keepdim=True)) / (mat.std(dim=0, keepdim=True))\n",
    "\n",
    "    EPOCHS=10\n",
    "    def fwd(self):\n",
    "        \n",
    "        #Positional embeddings here\n",
    "        Batch = oneHotTorch.mm(self.We)\n",
    "        Pos_emb = np.zeros(Batch.shape, dtype=float)\n",
    "        for p in range(len(SENTENCE.split())):\n",
    "            for i in range(DIM_EMB//2):\n",
    "                val = p/(10000**(2*i/DIM_EMB))\n",
    "                Pos_emb[p][2*i] = sin(val)\n",
    "                Pos_emb[p][2*i + 1] = cos(val)\n",
    "\n",
    "        Pos_emb_var = torch.nn.Parameter(torch.tensor(Pos_emb, dtype=torch.float32), requires_grad=False)\n",
    "\n",
    "        Batch = Batch*(DIM_EMB**(1/2))\n",
    "\n",
    "        Batch = Batch + Pos_emb_var\n",
    "\n",
    "        #Batch (in this case a one sentence batch) is prepared. Maxi calls this embedded sentence\n",
    "\n",
    "        #Create MASK HERE\n",
    "        #\n",
    "        mask = np.triu(np.ones((len(SENTENCE.split()), len(SENTENCE.split()))), k=1)\n",
    "        mask = torch.from_numpy(mask) == 0\n",
    "\n",
    "        transformer_head = self.add_transformer_head_with_mask(DIM_EMB, Batch, mask)\n",
    "        postAttention = self.addLinearProj(transformer_head, DIM_EMB)\n",
    "\n",
    "        # Add residual connection to the embeddings\n",
    "        postAttention = postAttention + Batch\n",
    "\n",
    "        normalizedPostAttention = self.layerNorm(postAttention)\n",
    "\n",
    "        # @TODO no dropout\n",
    "        # Take the output of the attention and do a one layer FF that outputs size of\n",
    "        activation = torch.nn.ReLU()\n",
    "        postFF = activation(normalizedPostAttention.mm(self.Wff) + self.Bff)\n",
    "\n",
    "        #Residual\n",
    "        preSoftmaxProj = self.layerNorm(postFF + normalizedPostAttention)\n",
    "\n",
    "        #Projexct\n",
    "\n",
    "        linearLay = activation(preSoftmaxProj.mm(self.Wproj) + self.Bproj)\n",
    "\n",
    "        #Softmax\n",
    "        softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        output = softmax(linearLay)\n",
    "        return output\n",
    "\n",
    "    #@TODO\n",
    "    def getOneHot(self, truth):\n",
    "        ret = torch.Tensor(self.batch_size).long() \n",
    "\n",
    "model = FUCKYOU(DICT_SIZE, len(SENTENCE.split()))\n",
    "EPOCHS=10\n",
    "\n",
    "def printSent(softmaxd, vocab):\n",
    "    words = []\n",
    "    for i in range(len(softmaxd)):\n",
    "        _, idx = torch.max(fwd[i], 0)\n",
    "        words.append(vocab[int(idx)])\n",
    "    print(\" \".join(words))\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.optimizer.zero_grad()\n",
    "    fwd = model.fwd()\n",
    "    printSent(fwd, inv_vocab)\n",
    "    truth = oneHotTorch.long() #SAKA [:, 1:].contiguous().view(-1)\n",
    "    truth = torch.Tensor([0, 1, 2, 3, 4, 5, 3, 6]).long()\n",
    "    loss = torch.nn.functional.cross_entropy(fwd, truth)\n",
    "    print(\"Loss: \", loss.item())\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
