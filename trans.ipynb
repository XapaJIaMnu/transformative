{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF WE WE ate LIKE LOVERS\n",
      "BOILED BOILED BOILED BOILED are AND AND\n",
      "OF LET OF Do LET OF TO\n",
      "OF BOILED OF BOILED THERE LET LET THERE\n",
      "like ? FREEDOM like Do like like GOOD OUT\n",
      "OUT OUT OF OF GLORY GLORY\n",
      "BOILED BOILED BOILED ate monkey EGG BOILED EGG are HARD\n",
      "monkey like LET AND like THERE THE ate THERE THE\n",
      "monkey monkey ALL HATERS HATERS ALL HATERS balloons balloons ALL <s> TO HATERS ALL\n",
      "Where Where THERE HATERS Where Where Where THERE the THERE HATERS the\n",
      "\n",
      "Loss:  48.42876434326172\n",
      "OF WE WE ate LIKE LOVERS\n",
      "BOILED BOILED BOILED BOILED are AND AND\n",
      "OF LET OF Do LET OF TO\n",
      "OF BOILED OF BOILED THERE LET LET THERE\n",
      "like ? FREEDOM like Do like like GOOD OUT\n",
      "OUT OUT OF OF GLORY GLORY\n",
      "BOILED BOILED BOILED ate monkey EGG BOILED EGG are HARD\n",
      "monkey like LET AND like THERE THE ate THERE THE\n",
      "monkey monkey ALL HATERS HATERS ALL HATERS balloons balloons ALL <s> TO HATERS ALL\n",
      "Where Where THERE HATERS Where Where Where THERE the THERE HATERS the\n",
      "\n",
      "Loss:  47.916996002197266\n",
      "OF WE WE ate LIKE LOVERS\n",
      "BOILED BOILED BOILED BOILED are AND THIS\n",
      "OF LET OF Do LET OF TO\n",
      "OF BOILED OF BOILED THERE LET LET THERE\n",
      "like ? FREEDOM like Do like like GOOD OUT\n",
      "OUT OUT OF OF GLORY GLORY\n",
      "BOILED BOILED BOILED ate monkey EGG BOILED EGG are HARD\n",
      "monkey like LET AND like THERE THE ate THERE THE\n",
      "monkey monkey ALL HATERS HATERS ALL HATERS balloons balloons ALL <s> TO HATERS ALL\n",
      "Where Where THERE HATERS Where Where Where THERE the THERE HATERS the\n",
      "\n",
      "Loss:  47.41040802001953\n",
      "OF WE WE ate LIKE LOVERS\n",
      "BOILED BOILED BOILED BOILED are AND THIS\n",
      "OF LET OF Do LET OF TO\n",
      "OF BOILED OF BOILED THERE LET LET THERE\n",
      "YOUNG ? FREEDOM like Do like like GOOD OUT\n",
      "OUT OUT OF OF GLORY GLORY\n",
      "BOILED BOILED BOILED ate monkey EGG BOILED EGG are HARD\n",
      "monkey monkey LET AND like THERE THE ate THERE THE\n",
      "monkey monkey ALL HATERS HATERS ALL HATERS balloons balloons ALL <s> TO HATERS ALL\n",
      "Where Where THERE HATERS Where Where Where THERE the THERE HATERS the\n",
      "\n",
      "Loss:  46.912025451660156\n",
      "OF WE WE ate LIKE LOVERS\n",
      "BOILED BOILED BOILED BOILED are AND THIS\n",
      "OF LET OF Do LET OF TO\n",
      "OF BOILED OF BOILED THERE LET LET THERE\n",
      "YOUNG ? FREEDOM like Do like like GOOD OUT\n",
      "OUT OUT OF OF GLORY GLORY\n",
      "BOILED BOILED BOILED hate monkey EGG BOILED EGG are HARD\n",
      "monkey monkey LET AND like THERE THE ate THERE THE\n",
      "monkey monkey ALL HATERS HATERS ALL HATERS balloons balloons ALL <s> TO HATERS ALL\n",
      "Where Where THERE HATERS Where Where Where THERE the THERE HATERS the\n",
      "\n",
      "Loss:  46.41609191894531\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import sin, cos\n",
    "from typing import List, Dict\n",
    "\n",
    "#FOR_GPU\n",
    "device = torch.device('cuda')\n",
    "#device = torch.device('cpu')\n",
    "RACHEL1 = \"<s> I hate cheese . </s>\"\n",
    "RACHEL2 = \"<s> Do you like cheese ? </s>\"\n",
    "RACHEL3 = \"<s> Where are the balloons ? </s>\"\n",
    "RACHEL4 = \"<s> A monkey ate my sandwich . </s>\"\n",
    "\n",
    "def getVocabDicts(sents: List[str]) -> (Dict[str, int], Dict[int, str]):\n",
    "    vocabDict = dict()\n",
    "    inv_vocab = dict()\n",
    "    i = 0\n",
    "    for sent in sents:\n",
    "        for token in sent.split():\n",
    "            if token not in vocabDict:\n",
    "                vocabDict[token] = i\n",
    "                i = i + 1\n",
    "    vocabDict[\"<unk>\"] = i\n",
    "    \n",
    "    for word in vocabDict:\n",
    "        inv_vocab[vocabDict[word]] = word\n",
    "    \n",
    "    return (vocabDict, inv_vocab)\n",
    "    \n",
    "\n",
    "SENTENCE = \"<s> I LIKE PIE VERY MUCH PIE . </s>\"\n",
    "SENTENCE2 = \"<s> WE ARE YOUNG . </s>\"\n",
    "SENTENCE3 = \"<s> LET US TEST HOW GOOD THIS IS . </s>\"\n",
    "SENTENCE4 = \"<s> TO SILENCE ALL THE HATERS OUT THERE . </s>\"\n",
    "SENTENCE5 = \"<s> AND FOR GLORY , FREEDOM , FREE PIE AND YOUNG LOVERS . </s>\"\n",
    "SENTENCE6 = \"<s> AND OF COURSE A HARD BOILED EGG IS ESSENTIAL . </s>\"\n",
    "DIM_EMB = 512\n",
    "torch.manual_seed(7)\n",
    "\n",
    "mysents = [SENTENCE, SENTENCE2, SENTENCE3, SENTENCE4, SENTENCE5, SENTENCE6]\n",
    "rachel_sents = [RACHEL1, RACHEL2, RACHEL3, RACHEL4]\n",
    "all_sents = rachel_sents + mysents\n",
    "vocab, inv_vocab = getVocabDicts(all_sents)\n",
    "\n",
    "DICT_SIZE = len(vocab)\n",
    "\n",
    "def getBatch(sents: List[str], vocab: Dict[str, int]) -> torch.Tensor:\n",
    "    #Get max sentence length\n",
    "    DICT_SIZE = len(vocab)\n",
    "    maxLen = 0\n",
    "    for sent in sents:\n",
    "        maxLen = max(maxLen, len(sent.split()))\n",
    "    oneHotBatch = np.zeros((len(sents), maxLen, DICT_SIZE), dtype=float)\n",
    "    for i in range(len(sents)):\n",
    "        curr_sent = sents[i].split()\n",
    "        for j in range(len(curr_sent)):\n",
    "            word = curr_sent[j]\n",
    "            oneHotBatch[i][j][vocab[word]] = 1\n",
    "    return torch.as_tensor(oneHotBatch, dtype=torch.float32, device=device)\n",
    "\n",
    "batch_tst = getBatch(all_sents, vocab)\n",
    "\n",
    "#oneHotSent = np.zeros((len(SENTENCE.split()), DICT_SIZE), dtype=float)\n",
    "\n",
    "#for i in range(len(SENTENCE.split())):\n",
    "#    word = SENTENCE.split()[i]\n",
    "#    oneHotSent[i][vocab[word]] = 1\n",
    "\n",
    "#oneHotTorch = torch.as_tensor(oneHotSent, dtype=torch.float32)\n",
    "    \n",
    "class FUCKYOU(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.DICT_SIZE = vocab_size\n",
    "        self.batch_size = batch_size #@TODO CHANGE\n",
    "        self.transformerHeadConcat = 64 #@TODO CHANGE\n",
    "        \n",
    "        Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wq = torch.nn.Parameter(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "        Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wk = torch.nn.Parameter(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "        Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wv = torch.nn.Parameter(Wv_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wo_skeleton = torch.tensor(np.zeros((self.transformerHeadConcat, DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wo = torch.nn.Parameter(Wo_skeleton, requires_grad=True)\n",
    "        \n",
    "        #Embeddings!\n",
    "        We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.We = torch.nn.Parameter(We_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wff_skeleton = torch.tensor(np.zeros((DIM_EMB, DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wff = torch.nn.Parameter(Wff_skeleton, requires_grad=True)\n",
    "        Bff_skeleton = torch.tensor(np.zeros((DIM_EMB), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Bff = torch.nn.Parameter(Bff_skeleton, requires_grad=True)\n",
    "        \n",
    "        Wproj_skeleton = torch.tensor(np.zeros((DIM_EMB, DICT_SIZE), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Wproj = torch.nn.Parameter(Wproj_skeleton, requires_grad=True)\n",
    "        Bproj_skeleton = torch.tensor(np.zeros((DICT_SIZE), dtype=float), dtype=torch.float32, device=device).normal_(0, 1)\n",
    "        self.Bproj = torch.nn.Parameter(Bproj_skeleton, requires_grad=True)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.0001)\n",
    "        \n",
    "    \n",
    "    #DATA PREPARATION DONE\n",
    "\n",
    "    # Create self attention\n",
    "    # also need to add a mask here\n",
    "    # @TODO Add a Bias to all Wsz\n",
    "    def add_transformer_head_with_mask(self, DIM_EMB, Batch, mask):\n",
    "        Q = Batch.matmul(self.Wq)\n",
    "        V = Batch.matmul(self.Wv)\n",
    "        K = Batch.matmul(self.Wk)\n",
    "\n",
    "        tmp1 = Q.matmul(K.permute(0, 2, 1))\n",
    "        #tmp1 = torch.t(K).matmul(Q)\n",
    "\n",
    "        tmp = tmp1/(64**(1/2))\n",
    "        #@TODO this might be wrong for dim=-1\n",
    "        softM = torch.nn.Softmax(dim=-1)\n",
    "        tmp = tmp.masked_fill(mask == 0, -1e9)\n",
    "        #print(\"AFTER MASK\", tmp)\n",
    "        a = softM(tmp)\n",
    "        #print(\"AFTER SOFTMAX\", a)\n",
    "        z = a.matmul(V)\n",
    "        #print(\"After Z*V\", z)\n",
    "        return z\n",
    "\n",
    "    # @TODO add a bias to Wo\n",
    "    def addLinearProj(self, concatTransHead, DIM_EMB):\n",
    "        '''Already concatenataed all heads'''\n",
    "\n",
    "        return concatTransHead.matmul(self.Wo)\n",
    "\n",
    "    #Normalize\n",
    "    def layerNorm(self, mat):\n",
    "        #@TODO there should be a square root here somewhere\n",
    "        return (mat - mat.mean(dim=1, keepdim=True)) / (mat.std(dim=1, keepdim=True))\n",
    "\n",
    "    EPOCHS=10\n",
    "    def fwd(self, oneHotTorch: torch.Tensor):\n",
    "        \n",
    "        #Positional embeddings here\n",
    "        Batch = oneHotTorch.matmul(self.We)\n",
    "        Pos_emb = np.zeros((Batch.shape[1], Batch.shape[2]), dtype=float)\n",
    "        for p in range(len(SENTENCE.split())):\n",
    "            for i in range(DIM_EMB//2):\n",
    "                val = p/(10000**(2*i/DIM_EMB))\n",
    "                Pos_emb[p][2*i] = sin(val)\n",
    "                Pos_emb[p][2*i + 1] = cos(val)\n",
    "\n",
    "        Pos_emb_var = torch.nn.Parameter(torch.tensor(Pos_emb, dtype=torch.float32, device=device), requires_grad=False)\n",
    "\n",
    "        Batch = Batch*(DIM_EMB**(1/2))\n",
    "        \n",
    "\n",
    "        Batch = Batch + Pos_emb_var\n",
    "        #Mask the batch post pos_emb\n",
    "        mask1 = torch.sum(oneHotTorch, dim=2) > 0\n",
    "        mask1 = mask1.reshape(Batch.shape[0], Batch.shape[1], 1).float()\n",
    "        \n",
    "        Batch = Batch*mask1\n",
    "\n",
    "        #Create MASK HERE\n",
    "        #\n",
    "        #print(Batch.shape)\n",
    "        mask = np.triu(np.ones((Batch.shape[1], Batch.shape[1])), k=1)\n",
    "        mask = torch.tensor(mask, device=device) == 0\n",
    "\n",
    "        transformer_head = self.add_transformer_head_with_mask(DIM_EMB, Batch, mask)\n",
    "        postAttention = self.addLinearProj(transformer_head, DIM_EMB)\n",
    "\n",
    "        # Add residual connection to the embeddings\n",
    "        postAttention = postAttention + Batch\n",
    "        #print(postAttention)\n",
    "\n",
    "        normalizedPostAttention = self.layerNorm(postAttention)\n",
    "        #print(normalizedPostAttention)\n",
    "        # @TODO no dropout\n",
    "        # Take the output of the attention and do a one layer FF that outputs size of\n",
    "        activation = torch.nn.ReLU()\n",
    "        postFF = activation(normalizedPostAttention.matmul(self.Wff) + self.Bff)\n",
    "\n",
    "        #Residual\n",
    "        preSoftmaxProj = self.layerNorm(postFF + normalizedPostAttention)\n",
    "\n",
    "        linearLay = preSoftmaxProj.matmul(self.Wproj) + self.Bproj\n",
    "        return linearLay\n",
    "\n",
    "    #@TODO\n",
    "    def getOneHot(self, truth):\n",
    "        ret = torch.Tensor(self.batch_size).long() \n",
    "\n",
    "model = FUCKYOU(DICT_SIZE, len(SENTENCE.split()))\n",
    "EPOCHS=5\n",
    "\n",
    "def printSent(softmaxd, vocab, batch_onehot):\n",
    "    words = [[]]\n",
    "    for j in range(len(softmaxd)):\n",
    "        words.append([])\n",
    "        for i in range(len(softmaxd[j])):\n",
    "            _, idx = torch.max(softmaxd[j][i], 0)\n",
    "            hasWord, _ = torch.max(batch_onehot[j][i], 0)\n",
    "            if hasWord == 1:\n",
    "                words[j].append(vocab[int(idx)])\n",
    "    for arr in words:\n",
    "        print(\" \".join(arr))\n",
    "    \n",
    "def getTruth(oneHotSent):\n",
    "    truth = []\n",
    "    for j in range(len(oneHotSent)):\n",
    "        truth.append([])\n",
    "        for i in range(len(oneHotSent[j])):\n",
    "            _, idx = torch.max(oneHotSent[j][i], 0)\n",
    "            truth[j].append(idx)\n",
    "    return torch.tensor(truth, device=device).long()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.optimizer.zero_grad()\n",
    "    fwd = model.fwd(batch_tst)\n",
    "    printSent(fwd, inv_vocab, batch_tst)\n",
    "    truth = getTruth(batch_tst)\n",
    "    loss = torch.nn.functional.cross_entropy(fwd.permute(0, 2, 1), truth, size_average = True, ignore_index=11)\n",
    "    print(\"Loss: \", loss.item())\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
