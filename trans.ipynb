{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'LIKE': 1, 'PIE': 2, 'VERY': 3, 'MUCH': 4, '.': 5, '<unk>': 6, '<s>': 7}\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([7, 64])\n",
      "torch.Size([7, 512])\n",
      "torch.Size([7, 8])\n",
      "tensor([[-3.7722e+01, -3.7722e+01, -2.8273e+01,  0.0000e+00, -3.7722e+01,\n",
      "         -1.6410e+01, -3.7722e+01, -3.7722e+01],\n",
      "        [-3.8353e+01, -3.8353e+01, -2.8882e+01,  0.0000e+00, -3.8353e+01,\n",
      "         -1.7881e+01, -3.8353e+01, -3.8353e+01],\n",
      "        [ 0.0000e+00, -2.9192e+01, -2.6355e+01, -3.7078e+01, -3.3044e+01,\n",
      "         -3.7078e+01, -2.2328e+01, -3.7078e+01],\n",
      "        [ 0.0000e+00, -4.0535e+01, -3.1988e+01, -4.3002e+01, -3.9004e+01,\n",
      "         -4.3002e+01, -2.9144e+01, -4.3002e+01],\n",
      "        [-3.5120e+01, -2.1042e+01, -3.5120e+01, -3.5120e+01, -1.7587e+01,\n",
      "         -3.5120e+01, -1.4799e+01,  0.0000e+00],\n",
      "        [-3.6240e-05, -1.6086e+01, -1.6555e+01, -2.0067e+01, -2.0067e+01,\n",
      "         -2.0067e+01, -1.0236e+01, -2.0067e+01],\n",
      "        [-3.8295e+01, -3.8295e+01, -2.9417e+01,  0.0000e+00, -3.8295e+01,\n",
      "         -1.5883e+01, -3.8295e+01, -3.8295e+01]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import sin, cos\n",
    "\n",
    "SENTENCE = \"I LIKE PIE VERY MUCH PIE .\"\n",
    "DIM_EMB = 512\n",
    "torch.manual_seed(7)\n",
    "\n",
    "vocab = dict()\n",
    "i = 0\n",
    "for token in SENTENCE.split():\n",
    "    if token not in vocab:\n",
    "        vocab[token] = i\n",
    "        i += 1\n",
    "vocab[\"<unk>\"] = i\n",
    "vocab[\"<s>\"] = i + 1\n",
    "print(vocab)\n",
    "\n",
    "DICT_SIZE = len(vocab)\n",
    "\n",
    "oneHotSent = np.zeros((len(SENTENCE.split()), DICT_SIZE), dtype=float)\n",
    "\n",
    "for i in range(len(SENTENCE.split())):\n",
    "    word = SENTENCE.split()[i]\n",
    "    oneHotSent[i][vocab[word]] = 1\n",
    "\n",
    "oneHotTorch = torch.as_tensor(oneHotSent, dtype=torch.float32)\n",
    "    \n",
    "#print(oneHotTorch)\n",
    "#print(oneHotTorch.shape)\n",
    "#print(oneHotTorch.dtype)\n",
    "\n",
    "#DATA PREPARATION DONE\n",
    "\n",
    "We_skeleton = torch.tensor(np.zeros((DICT_SIZE, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "We = torch.autograd.Variable(We_skeleton, requires_grad=True)\n",
    "#print(We)\n",
    "#print(We.shape)\n",
    "#print(We.dtype)\n",
    "\n",
    "\n",
    "Batch = oneHotTorch.mm(We)\n",
    "Pos_emb = np.zeros(Batch.shape, dtype=float)\n",
    "for p in range(len(SENTENCE.split())):\n",
    "    for i in range(DIM_EMB//2):\n",
    "        val = p/(10000**(2*i/DIM_EMB))\n",
    "        Pos_emb[p][2*i] = sin(val)\n",
    "        Pos_emb[p][2*i + 1] = cos(val)\n",
    "\n",
    "Pos_emb_var = torch.autograd.Variable(torch.tensor(Pos_emb, dtype=torch.float32), requires_grad=False)\n",
    "#print(Pos_emb_var)\n",
    "\n",
    "#print(Batch)\n",
    "#Scale batch up\n",
    "Batch = Batch*(DIM_EMB**(1/2))\n",
    "#print(Batch)\n",
    "Batch = Batch + Pos_emb_var\n",
    "#print(Batch)\n",
    "\n",
    "#print(Batch.shape)\n",
    "#print(Batch.dtype)\n",
    "\n",
    "#Batch (in this case a one sentence batch) is prepared. Maxi calls this embedded sentence\n",
    "\n",
    "#Create MASK HERE\n",
    "#\n",
    "mask = np.triu(np.ones((len(SENTENCE.split()), len(SENTENCE.split()))), k=1)\n",
    "mask = torch.from_numpy(mask) == 0\n",
    "print(mask)\n",
    "\n",
    "# Create self attention\n",
    "# also need to add a mask here\n",
    "# @TODO Add a Bias to all Wsz\n",
    "def add_transformer_head_with_mask(DIM_EMB, Batch, mask):\n",
    "    Wq_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wq = torch.autograd.Variable(Wq_skeleton, requires_grad=True)\n",
    "\n",
    "    Wk_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wk = torch.autograd.Variable(Wk_skeleton, requires_grad=True)\n",
    "\n",
    "    Wv_skeleton = torch.tensor(np.zeros((DIM_EMB, 64), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wv = torch.autograd.Variable(Wv_skeleton, requires_grad=True)\n",
    "\n",
    "    Q = Batch.mm(Wq)\n",
    "    V = Batch.mm(Wv)\n",
    "    K = Batch.mm(Wk)\n",
    "    \n",
    "    tmp1 = Q.mm(torch.t(K))\n",
    "\n",
    "    tmp = tmp1/(64**(1/2))\n",
    "    #print(tmp)\n",
    "    \n",
    "    softM = torch.nn.Softmax(dim=-1)\n",
    "    #apply maks here\n",
    "    tmp = tmp.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    #print(tmp)\n",
    "\n",
    "    \n",
    "    a = softM(tmp)\n",
    "    print(a)\n",
    "\n",
    "    z = a.mm(V)\n",
    "    return z\n",
    "\n",
    "# @TODO add a bias to Wo\n",
    "def addLinearProj(concatTransHead, DIM_EMB):\n",
    "    '''Already concatenataed all heads'''\n",
    "    Wo_skeleton = torch.tensor(np.zeros((concatTransHead.shape[1], DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "    Wo = torch.autograd.Variable(Wo_skeleton, requires_grad=True)\n",
    "    \n",
    "    return concatTransHead.mm(Wo)\n",
    "    \n",
    "\n",
    "transformer_head = add_transformer_head_with_mask(DIM_EMB, Batch, mask)\n",
    "postAttention = addLinearProj(transformer_head, DIM_EMB)\n",
    "print(transformer_head.shape)\n",
    "print(postAttention.shape)\n",
    "\n",
    "# Add residual connection to the embeddings\n",
    "postAttention = postAttention + Batch\n",
    "\n",
    "#Normalize\n",
    "\n",
    "def layerNorm(mat):\n",
    "    return (mat - mat.mean(dim=0, keepdim=True)) / (mat.std(dim=0, keepdim=True))\n",
    "\n",
    "normalizedPostAttention = layerNorm(postAttention)\n",
    "\n",
    "# @TODO no dropout\n",
    "# Take the output of the attention and do a one layer FF that outputs size of\n",
    "Wff_skeleton = torch.tensor(np.zeros((DIM_EMB, DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "Wff = torch.autograd.Variable(Wff_skeleton, requires_grad=True)\n",
    "Bff_skeleton = torch.tensor(np.zeros((DIM_EMB), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "Bff = torch.autograd.Variable(Bff_skeleton, requires_grad=True)\n",
    "\n",
    "activation = torch.nn.ReLU()\n",
    "postFF = activation(normalizedPostAttention.mm(Wff) + Bff)\n",
    "\n",
    "#Residual\n",
    "preSoftmaxProj = layerNorm(postFF + normalizedPostAttention)\n",
    "\n",
    "#Projexct\n",
    "Wproj_skeleton = torch.tensor(np.zeros((DIM_EMB, DICT_SIZE), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "Wproj = torch.autograd.Variable(Wproj_skeleton, requires_grad=True)\n",
    "Bproj_skeleton = torch.tensor(np.zeros((DICT_SIZE), dtype=float), dtype=torch.float32,).normal_(0, 1)\n",
    "Bproj = torch.autograd.Variable(Bproj_skeleton, requires_grad=True)\n",
    "\n",
    "linearLay = activation(preSoftmaxProj.mm(Wproj) + Bproj)\n",
    "\n",
    "#Softmax\n",
    "softmax = torch.nn.LogSoftmax(dim=1)\n",
    "output = softmax(linearLay)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
